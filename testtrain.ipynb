{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.dont_write_bytecode = True\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import export\n",
    "from models import *\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = LSTMForNLP(50257, 128, 64, 2, True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), 1e-1, weight_decay=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(x):\n",
    "    tokens = torch.Tensor([tokenizer.encode(x[0]) + [ 26 ] + tokenizer.encode(x[1]) + [ 50256 ]]).long()\n",
    "    return { \"input_ids\": tokens[:, :-1], \"labels\": tokens[:, 1:] }\n",
    "\n",
    "dataset = [ prepare(x) for x in pd.read_csv(\"questions.csv\").to_numpy() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [ prepare([\"Hello\", \"bye\"]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t train loss: 10.840571403503418\n",
      "Epoch: 1 \t train loss: 8.43688678741455\n",
      "Epoch: 2 \t train loss: 2.9861838817596436\n",
      "Epoch: 3 \t train loss: 0.24526067078113556\n",
      "Epoch: 4 \t train loss: 0.020272158086299896\n",
      "Epoch: 5 \t train loss: 0.005461444612592459\n",
      "Epoch: 6 \t train loss: 0.003999364096671343\n",
      "Epoch: 7 \t train loss: 0.0033545626793056726\n",
      "Epoch: 8 \t train loss: 0.002719772281125188\n",
      "Epoch: 9 \t train loss: 0.002066652989014983\n",
      "Epoch: 10 \t train loss: 0.0015058689750730991\n",
      "Epoch: 11 \t train loss: 0.0010789656080305576\n",
      "Epoch: 12 \t train loss: 0.0007736051338724792\n",
      "Epoch: 13 \t train loss: 0.0005608846549876034\n",
      "Epoch: 14 \t train loss: 0.0004135586495976895\n",
      "Epoch: 15 \t train loss: 0.0003111102560069412\n",
      "Epoch: 16 \t train loss: 0.0002390084118815139\n",
      "Epoch: 17 \t train loss: 0.0001875032321549952\n",
      "Epoch: 18 \t train loss: 0.00015005072054918855\n",
      "Epoch: 19 \t train loss: 0.0001223657454829663\n",
      "Epoch: 20 \t train loss: 0.00010155083873542026\n",
      "Epoch: 21 \t train loss: 8.566072938265279e-05\n",
      "Epoch: 22 \t train loss: 7.330565131269395e-05\n",
      "Epoch: 23 \t train loss: 6.357222446240485e-05\n",
      "Epoch: 24 \t train loss: 5.582500671152957e-05\n",
      "Epoch: 25 \t train loss: 4.9587379180593416e-05\n",
      "Epoch: 26 \t train loss: 4.450183405424468e-05\n",
      "Epoch: 27 \t train loss: 4.029030606034212e-05\n",
      "Epoch: 28 \t train loss: 3.683363684103824e-05\n",
      "Epoch: 29 \t train loss: 3.3893451472977176e-05\n",
      "Epoch: 30 \t train loss: 3.14300341415219e-05\n",
      "Epoch: 31 \t train loss: 2.936392957053613e-05\n",
      "Epoch: 32 \t train loss: 2.7575943022384308e-05\n",
      "Epoch: 33 \t train loss: 2.606607995403465e-05\n",
      "Epoch: 34 \t train loss: 2.471514198987279e-05\n",
      "Epoch: 35 \t train loss: 2.3602602595929056e-05\n",
      "Epoch: 36 \t train loss: 2.2609261577599682e-05\n",
      "Epoch: 37 \t train loss: 2.1735118934884667e-05\n",
      "Epoch: 38 \t train loss: 2.0980174667784013e-05\n",
      "Epoch: 39 \t train loss: 2.0304696590756066e-05\n",
      "Epoch: 40 \t train loss: 1.9748420527321286e-05\n",
      "Epoch: 41 \t train loss: 1.923187664942816e-05\n",
      "Epoch: 42 \t train loss: 1.8755066776066087e-05\n",
      "Epoch: 43 \t train loss: 1.8357724911766127e-05\n",
      "Epoch: 44 \t train loss: 1.8000117051997222e-05\n",
      "Epoch: 45 \t train loss: 1.768224137776997e-05\n",
      "Epoch: 46 \t train loss: 1.7443837350583635e-05\n",
      "Epoch: 47 \t train loss: 1.716569568088744e-05\n",
      "Epoch: 48 \t train loss: 1.6967023839242756e-05\n",
      "Epoch: 49 \t train loss: 1.6768351997598074e-05\n",
      "Epoch: 50 \t train loss: 1.6609414160484448e-05\n",
      "Epoch: 51 \t train loss: 1.6450476323370822e-05\n",
      "Epoch: 52 \t train loss: 1.633127249078825e-05\n",
      "Epoch: 53 \t train loss: 1.621206865820568e-05\n",
      "Epoch: 54 \t train loss: 1.6092866644612513e-05\n",
      "Epoch: 55 \t train loss: 1.6013396816561e-05\n",
      "Epoch: 56 \t train loss: 1.5933926988509484e-05\n",
      "Epoch: 57 \t train loss: 1.5894192983978428e-05\n",
      "Epoch: 58 \t train loss: 1.5814724974916317e-05\n",
      "Epoch: 59 \t train loss: 1.5774989151395857e-05\n",
      "Epoch: 60 \t train loss: 1.57352551468648e-05\n",
      "Epoch: 61 \t train loss: 1.5695519323344342e-05\n",
      "Epoch: 62 \t train loss: 1.5655785318813287e-05\n",
      "Epoch: 63 \t train loss: 1.561605131428223e-05\n",
      "Epoch: 64 \t train loss: 1.561605131428223e-05\n",
      "Epoch: 65 \t train loss: 1.5576317309751175e-05\n",
      "Epoch: 66 \t train loss: 1.5576317309751175e-05\n",
      "Epoch: 67 \t train loss: 1.5576317309751175e-05\n",
      "Epoch: 68 \t train loss: 1.5576317309751175e-05\n",
      "Epoch: 69 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 70 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 71 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 72 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 73 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 74 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 75 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 76 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 77 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 78 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 79 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 80 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 81 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 82 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 83 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 84 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 85 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 86 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 87 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 88 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 89 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 90 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 91 \t train loss: 1.5536581486230716e-05\n",
      "Epoch: 92 \t train loss: 1.549684748169966e-05\n",
      "Epoch: 93 \t train loss: 1.549684748169966e-05\n",
      "Epoch: 94 \t train loss: 1.549684748169966e-05\n",
      "Epoch: 95 \t train loss: 1.549684748169966e-05\n",
      "Epoch: 96 \t train loss: 1.54571116581792e-05\n",
      "Epoch: 97 \t train loss: 1.54571116581792e-05\n",
      "Epoch: 98 \t train loss: 1.5417377653648145e-05\n",
      "Epoch: 99 \t train loss: 1.5417377653648145e-05\n",
      "Epoch: 100 \t train loss: 1.537764364911709e-05\n",
      "Epoch: 101 \t train loss: 1.537764364911709e-05\n",
      "Epoch: 102 \t train loss: 1.5337909644586034e-05\n",
      "Epoch: 103 \t train loss: 1.529817564005498e-05\n",
      "Epoch: 104 \t train loss: 1.525843981653452e-05\n",
      "Epoch: 105 \t train loss: 1.525843981653452e-05\n",
      "Epoch: 106 \t train loss: 1.5218705812003464e-05\n",
      "Epoch: 107 \t train loss: 1.5178970897977706e-05\n",
      "Epoch: 108 \t train loss: 1.5139235074457247e-05\n",
      "Epoch: 109 \t train loss: 1.5099501069926191e-05\n",
      "Epoch: 110 \t train loss: 1.5059766155900434e-05\n",
      "Epoch: 111 \t train loss: 1.498029723734362e-05\n",
      "Epoch: 112 \t train loss: 1.4940563232812565e-05\n",
      "Epoch: 113 \t train loss: 1.4900827409292106e-05\n",
      "Epoch: 114 \t train loss: 1.486109340476105e-05\n",
      "Epoch: 115 \t train loss: 1.4781624486204237e-05\n",
      "Epoch: 116 \t train loss: 1.474188957217848e-05\n",
      "Epoch: 117 \t train loss: 1.4662419744126964e-05\n",
      "Epoch: 118 \t train loss: 1.4582950825570151e-05\n",
      "Epoch: 119 \t train loss: 1.4543215911544394e-05\n",
      "Epoch: 120 \t train loss: 1.4463746083492879e-05\n",
      "Epoch: 121 \t train loss: 1.4384277164936066e-05\n",
      "Epoch: 122 \t train loss: 1.4304808246379253e-05\n",
      "Epoch: 123 \t train loss: 1.4225338418327738e-05\n",
      "Epoch: 124 \t train loss: 1.4145868590276223e-05\n",
      "Epoch: 125 \t train loss: 1.4026664757693652e-05\n",
      "Epoch: 126 \t train loss: 1.3947194929642137e-05\n",
      "Epoch: 127 \t train loss: 1.3827990187564865e-05\n",
      "Epoch: 128 \t train loss: 1.3748521269008052e-05\n",
      "Epoch: 129 \t train loss: 1.362931652693078e-05\n",
      "Epoch: 130 \t train loss: 1.3510111784853507e-05\n",
      "Epoch: 131 \t train loss: 1.3390907952270936e-05\n",
      "Epoch: 132 \t train loss: 1.3271703210193664e-05\n",
      "Epoch: 133 \t train loss: 1.3152498468116391e-05\n",
      "Epoch: 134 \t train loss: 1.303329463553382e-05\n",
      "Epoch: 135 \t train loss: 1.2914089893456548e-05\n",
      "Epoch: 136 \t train loss: 1.2794884241884574e-05\n",
      "Epoch: 137 \t train loss: 1.2675679499807302e-05\n",
      "Epoch: 138 \t train loss: 1.2556475667224731e-05\n",
      "Epoch: 139 \t train loss: 1.2477004929678515e-05\n",
      "Epoch: 140 \t train loss: 1.2357800187601242e-05\n",
      "Epoch: 141 \t train loss: 1.2278331269044429e-05\n",
      "Epoch: 142 \t train loss: 1.2198860531498212e-05\n",
      "Epoch: 143 \t train loss: 1.2079656698915642e-05\n",
      "Epoch: 144 \t train loss: 1.2039920875395183e-05\n",
      "Epoch: 145 \t train loss: 1.1960451047343668e-05\n",
      "Epoch: 146 \t train loss: 1.1880980309797451e-05\n",
      "Epoch: 147 \t train loss: 1.1801511391240638e-05\n",
      "Epoch: 148 \t train loss: 1.1722040653694421e-05\n",
      "Epoch: 149 \t train loss: 1.1682305739668664e-05\n",
      "Epoch: 150 \t train loss: 1.1602835002122447e-05\n",
      "Epoch: 151 \t train loss: 1.1523366083565634e-05\n",
      "Epoch: 152 \t train loss: 1.1443895346019417e-05\n",
      "Epoch: 153 \t train loss: 1.140416043199366e-05\n",
      "Epoch: 154 \t train loss: 1.1324689694447443e-05\n",
      "Epoch: 155 \t train loss: 1.124522077589063e-05\n",
      "Epoch: 156 \t train loss: 1.120548495237017e-05\n",
      "Epoch: 157 \t train loss: 1.1126015124318656e-05\n",
      "Epoch: 158 \t train loss: 1.1086279300798196e-05\n",
      "Epoch: 159 \t train loss: 1.1006809472746681e-05\n",
      "Epoch: 160 \t train loss: 1.0967073649226222e-05\n",
      "Epoch: 161 \t train loss: 1.0927338735200465e-05\n",
      "Epoch: 162 \t train loss: 1.0847867997654248e-05\n",
      "Epoch: 163 \t train loss: 1.0808133993123192e-05\n",
      "Epoch: 164 \t train loss: 1.0768399079097435e-05\n",
      "Epoch: 165 \t train loss: 1.0728664165071677e-05\n",
      "Epoch: 166 \t train loss: 1.0688928341551218e-05\n",
      "Epoch: 167 \t train loss: 1.064919342752546e-05\n",
      "Epoch: 168 \t train loss: 1.0609458513499703e-05\n",
      "Epoch: 169 \t train loss: 1.0569722689979244e-05\n",
      "Epoch: 170 \t train loss: 1.0529987775953487e-05\n",
      "Epoch: 171 \t train loss: 1.0490252861927729e-05\n",
      "Epoch: 172 \t train loss: 1.0490252861927729e-05\n",
      "Epoch: 173 \t train loss: 1.045051703840727e-05\n",
      "Epoch: 174 \t train loss: 1.0410782124381512e-05\n",
      "Epoch: 175 \t train loss: 1.0410782124381512e-05\n",
      "Epoch: 176 \t train loss: 1.0371047210355755e-05\n",
      "Epoch: 177 \t train loss: 1.0331311386835296e-05\n",
      "Epoch: 178 \t train loss: 1.0331311386835296e-05\n",
      "Epoch: 179 \t train loss: 1.0291576472809538e-05\n",
      "Epoch: 180 \t train loss: 1.025184155878378e-05\n",
      "Epoch: 181 \t train loss: 1.025184155878378e-05\n",
      "Epoch: 182 \t train loss: 1.0212105735263322e-05\n",
      "Epoch: 183 \t train loss: 1.0212105735263322e-05\n",
      "Epoch: 184 \t train loss: 1.0212105735263322e-05\n",
      "Epoch: 185 \t train loss: 1.0172370821237564e-05\n",
      "Epoch: 186 \t train loss: 1.0172370821237564e-05\n",
      "Epoch: 187 \t train loss: 1.0132635907211807e-05\n",
      "Epoch: 188 \t train loss: 1.0132635907211807e-05\n",
      "Epoch: 189 \t train loss: 1.0092900083691347e-05\n",
      "Epoch: 190 \t train loss: 1.0092900083691347e-05\n",
      "Epoch: 191 \t train loss: 1.0092900083691347e-05\n",
      "Epoch: 192 \t train loss: 1.005316516966559e-05\n",
      "Epoch: 193 \t train loss: 1.005316516966559e-05\n",
      "Epoch: 194 \t train loss: 1.005316516966559e-05\n",
      "Epoch: 195 \t train loss: 1.0092901902680751e-05\n",
      "Epoch: 196 \t train loss: 1.0053166079160292e-05\n",
      "Epoch: 197 \t train loss: 1.0053166079160292e-05\n",
      "Epoch: 198 \t train loss: 1.0053166079160292e-05\n",
      "Epoch: 199 \t train loss: 1.0053166079160292e-05\n",
      "Epoch: 200 \t train loss: 1.0013431165134534e-05\n",
      "Epoch: 201 \t train loss: 1.0013431165134534e-05\n",
      "Epoch: 202 \t train loss: 1.0013431165134534e-05\n",
      "Epoch: 203 \t train loss: 1.0013431165134534e-05\n",
      "Epoch: 204 \t train loss: 9.973696251108777e-06\n",
      "Epoch: 205 \t train loss: 9.973696251108777e-06\n",
      "Epoch: 206 \t train loss: 9.973696251108777e-06\n",
      "Epoch: 207 \t train loss: 9.973696251108777e-06\n",
      "Epoch: 208 \t train loss: 9.973696251108777e-06\n",
      "Epoch: 209 \t train loss: 9.973696251108777e-06\n",
      "Epoch: 210 \t train loss: 9.933960427588318e-06\n",
      "Epoch: 211 \t train loss: 9.933960427588318e-06\n",
      "Epoch: 212 \t train loss: 9.933960427588318e-06\n",
      "Epoch: 213 \t train loss: 9.933960427588318e-06\n",
      "Epoch: 214 \t train loss: 9.933960427588318e-06\n",
      "Epoch: 215 \t train loss: 9.933960427588318e-06\n",
      "Epoch: 216 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 217 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 218 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 219 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 220 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 221 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 222 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 223 \t train loss: 9.89422551356256e-06\n",
      "Epoch: 224 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 225 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 226 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 227 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 228 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 229 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 230 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 231 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 232 \t train loss: 9.854490599536803e-06\n",
      "Epoch: 233 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 234 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 235 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 236 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 237 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 238 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 239 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 240 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 241 \t train loss: 9.814754776016343e-06\n",
      "Epoch: 242 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 243 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 244 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 245 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 246 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 247 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 248 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 249 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 250 \t train loss: 9.775019861990586e-06\n",
      "Epoch: 251 \t train loss: 9.735284038470127e-06\n",
      "Epoch: 252 \t train loss: 9.735284038470127e-06\n",
      "Epoch: 253 \t train loss: 9.735284038470127e-06\n",
      "Epoch: 254 \t train loss: 9.775020771485288e-06\n",
      "Epoch: 255 \t train loss: 9.775020771485288e-06\n",
      "Epoch: 256 \t train loss: 9.775020771485288e-06\n",
      "Epoch: 257 \t train loss: 9.775020771485288e-06\n",
      "Epoch: 258 \t train loss: 9.775020771485288e-06\n",
      "Epoch: 259 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 260 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 261 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 262 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 263 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 264 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 265 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 266 \t train loss: 9.735284947964828e-06\n",
      "Epoch: 267 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 268 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 269 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 270 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 271 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 272 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 273 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 274 \t train loss: 9.695550033939071e-06\n",
      "Epoch: 275 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 276 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 277 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 278 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 279 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 280 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 281 \t train loss: 9.655815119913314e-06\n",
      "Epoch: 282 \t train loss: 9.616079296392854e-06\n",
      "Epoch: 283 \t train loss: 9.655816029408015e-06\n",
      "Epoch: 284 \t train loss: 9.655816029408015e-06\n",
      "Epoch: 285 \t train loss: 9.655816029408015e-06\n",
      "Epoch: 286 \t train loss: 9.655816029408015e-06\n",
      "Epoch: 287 \t train loss: 9.655816029408015e-06\n",
      "Epoch: 288 \t train loss: 9.616081115382258e-06\n",
      "Epoch: 289 \t train loss: 9.616081115382258e-06\n",
      "Epoch: 290 \t train loss: 9.616081115382258e-06\n",
      "Epoch: 291 \t train loss: 9.616081115382258e-06\n",
      "Epoch: 292 \t train loss: 9.616081115382258e-06\n",
      "Epoch: 293 \t train loss: 9.616081115382258e-06\n",
      "Epoch: 294 \t train loss: 9.576345291861799e-06\n",
      "Epoch: 295 \t train loss: 9.576345291861799e-06\n",
      "Epoch: 296 \t train loss: 9.576345291861799e-06\n",
      "Epoch: 297 \t train loss: 9.576345291861799e-06\n",
      "Epoch: 298 \t train loss: 9.576345291861799e-06\n",
      "Epoch: 299 \t train loss: 9.576345291861799e-06\n",
      "Epoch: 300 \t train loss: 9.536610377836041e-06\n",
      "Epoch: 301 \t train loss: 9.536610377836041e-06\n",
      "Epoch: 302 \t train loss: 9.536610377836041e-06\n",
      "Epoch: 303 \t train loss: 9.576347110851202e-06\n",
      "Epoch: 304 \t train loss: 9.576347110851202e-06\n",
      "Epoch: 305 \t train loss: 9.536611287330743e-06\n",
      "Epoch: 306 \t train loss: 9.536611287330743e-06\n",
      "Epoch: 307 \t train loss: 9.536611287330743e-06\n",
      "Epoch: 308 \t train loss: 9.536611287330743e-06\n",
      "Epoch: 309 \t train loss: 9.536611287330743e-06\n",
      "Epoch: 310 \t train loss: 9.496875463810284e-06\n",
      "Epoch: 311 \t train loss: 9.496875463810284e-06\n",
      "Epoch: 312 \t train loss: 9.496875463810284e-06\n",
      "Epoch: 313 \t train loss: 9.496875463810284e-06\n",
      "Epoch: 314 \t train loss: 9.496875463810284e-06\n",
      "Epoch: 315 \t train loss: 9.496875463810284e-06\n",
      "Epoch: 316 \t train loss: 9.457140549784526e-06\n",
      "Epoch: 317 \t train loss: 9.457140549784526e-06\n",
      "Epoch: 318 \t train loss: 9.496877282799687e-06\n",
      "Epoch: 319 \t train loss: 9.496877282799687e-06\n",
      "Epoch: 320 \t train loss: 9.457141459279228e-06\n",
      "Epoch: 321 \t train loss: 9.457141459279228e-06\n",
      "Epoch: 322 \t train loss: 9.457141459279228e-06\n",
      "Epoch: 323 \t train loss: 9.457141459279228e-06\n",
      "Epoch: 324 \t train loss: 9.457141459279228e-06\n",
      "Epoch: 325 \t train loss: 9.41740654525347e-06\n",
      "Epoch: 326 \t train loss: 9.41740654525347e-06\n",
      "Epoch: 327 \t train loss: 9.41740654525347e-06\n",
      "Epoch: 328 \t train loss: 9.41740654525347e-06\n",
      "Epoch: 329 \t train loss: 9.41740654525347e-06\n",
      "Epoch: 330 \t train loss: 9.377671631227713e-06\n",
      "Epoch: 331 \t train loss: 9.417407454748172e-06\n",
      "Epoch: 332 \t train loss: 9.417407454748172e-06\n",
      "Epoch: 333 \t train loss: 9.417407454748172e-06\n",
      "Epoch: 334 \t train loss: 9.417407454748172e-06\n",
      "Epoch: 335 \t train loss: 9.377671631227713e-06\n",
      "Epoch: 336 \t train loss: 9.377671631227713e-06\n",
      "Epoch: 337 \t train loss: 9.377671631227713e-06\n",
      "Epoch: 338 \t train loss: 9.377671631227713e-06\n",
      "Epoch: 339 \t train loss: 9.337936717201956e-06\n",
      "Epoch: 340 \t train loss: 9.337936717201956e-06\n",
      "Epoch: 341 \t train loss: 9.377673450217117e-06\n",
      "Epoch: 342 \t train loss: 9.377673450217117e-06\n",
      "Epoch: 343 \t train loss: 9.377673450217117e-06\n",
      "Epoch: 344 \t train loss: 9.337937626696657e-06\n",
      "Epoch: 345 \t train loss: 9.337937626696657e-06\n",
      "Epoch: 346 \t train loss: 9.337937626696657e-06\n",
      "Epoch: 347 \t train loss: 9.337937626696657e-06\n",
      "Epoch: 348 \t train loss: 9.2982027126709e-06\n",
      "Epoch: 349 \t train loss: 9.2982027126709e-06\n",
      "Epoch: 350 \t train loss: 9.2982027126709e-06\n",
      "Epoch: 351 \t train loss: 9.337939445686061e-06\n",
      "Epoch: 352 \t train loss: 9.337939445686061e-06\n",
      "Epoch: 353 \t train loss: 9.298203622165602e-06\n",
      "Epoch: 354 \t train loss: 9.298203622165602e-06\n",
      "Epoch: 355 \t train loss: 9.298203622165602e-06\n",
      "Epoch: 356 \t train loss: 9.298203622165602e-06\n",
      "Epoch: 357 \t train loss: 9.258467798645142e-06\n",
      "Epoch: 358 \t train loss: 9.258467798645142e-06\n",
      "Epoch: 359 \t train loss: 9.298204531660303e-06\n",
      "Epoch: 360 \t train loss: 9.298204531660303e-06\n",
      "Epoch: 361 \t train loss: 9.258469617634546e-06\n",
      "Epoch: 362 \t train loss: 9.258469617634546e-06\n",
      "Epoch: 363 \t train loss: 9.258469617634546e-06\n",
      "Epoch: 364 \t train loss: 9.258469617634546e-06\n",
      "Epoch: 365 \t train loss: 9.258469617634546e-06\n",
      "Epoch: 366 \t train loss: 9.258470527129248e-06\n",
      "Epoch: 367 \t train loss: 9.258470527129248e-06\n",
      "Epoch: 368 \t train loss: 9.258470527129248e-06\n",
      "Epoch: 369 \t train loss: 9.258470527129248e-06\n",
      "Epoch: 370 \t train loss: 9.218734703608789e-06\n",
      "Epoch: 371 \t train loss: 9.218734703608789e-06\n",
      "Epoch: 372 \t train loss: 9.218734703608789e-06\n",
      "Epoch: 373 \t train loss: 9.258470527129248e-06\n",
      "Epoch: 374 \t train loss: 9.258470527129248e-06\n",
      "Epoch: 375 \t train loss: 9.21873561310349e-06\n",
      "Epoch: 376 \t train loss: 9.21873561310349e-06\n",
      "Epoch: 377 \t train loss: 9.21873561310349e-06\n",
      "Epoch: 378 \t train loss: 9.21873561310349e-06\n",
      "Epoch: 379 \t train loss: 9.218736522598192e-06\n",
      "Epoch: 380 \t train loss: 9.218736522598192e-06\n",
      "Epoch: 381 \t train loss: 9.218736522598192e-06\n",
      "Epoch: 382 \t train loss: 9.218736522598192e-06\n",
      "Epoch: 383 \t train loss: 9.179001608572435e-06\n",
      "Epoch: 384 \t train loss: 9.218737432092894e-06\n",
      "Epoch: 385 \t train loss: 9.218737432092894e-06\n",
      "Epoch: 386 \t train loss: 9.218737432092894e-06\n",
      "Epoch: 387 \t train loss: 9.218737432092894e-06\n",
      "Epoch: 388 \t train loss: 9.179001608572435e-06\n",
      "Epoch: 389 \t train loss: 9.179001608572435e-06\n",
      "Epoch: 390 \t train loss: 9.218738341587596e-06\n",
      "Epoch: 391 \t train loss: 9.218738341587596e-06\n",
      "Epoch: 392 \t train loss: 9.179003427561838e-06\n",
      "Epoch: 393 \t train loss: 9.179003427561838e-06\n",
      "Epoch: 394 \t train loss: 9.179003427561838e-06\n",
      "Epoch: 395 \t train loss: 9.218739251082297e-06\n",
      "Epoch: 396 \t train loss: 9.218739251082297e-06\n",
      "Epoch: 397 \t train loss: 9.17900433705654e-06\n",
      "Epoch: 398 \t train loss: 9.17900433705654e-06\n",
      "Epoch: 399 \t train loss: 9.218741070071701e-06\n",
      "Epoch: 400 \t train loss: 9.218741070071701e-06\n",
      "Epoch: 401 \t train loss: 9.218741070071701e-06\n",
      "Epoch: 402 \t train loss: 9.179005246551242e-06\n",
      "Epoch: 403 \t train loss: 9.179005246551242e-06\n",
      "Epoch: 404 \t train loss: 9.218741070071701e-06\n",
      "Epoch: 405 \t train loss: 9.218741070071701e-06\n",
      "Epoch: 406 \t train loss: 9.179006156045943e-06\n",
      "Epoch: 407 \t train loss: 9.179006156045943e-06\n",
      "Epoch: 408 \t train loss: 9.218742889061105e-06\n",
      "Epoch: 409 \t train loss: 9.218742889061105e-06\n",
      "Epoch: 410 \t train loss: 9.218742889061105e-06\n",
      "Epoch: 411 \t train loss: 9.179007065540645e-06\n",
      "Epoch: 412 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 413 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 414 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 415 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 416 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 417 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 418 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 419 \t train loss: 9.218743798555806e-06\n",
      "Epoch: 420 \t train loss: 9.218744708050508e-06\n",
      "Epoch: 421 \t train loss: 9.218744708050508e-06\n",
      "Epoch: 422 \t train loss: 9.218744708050508e-06\n",
      "Epoch: 423 \t train loss: 9.258481441065669e-06\n",
      "Epoch: 424 \t train loss: 9.258481441065669e-06\n",
      "Epoch: 425 \t train loss: 9.218746527039912e-06\n",
      "Epoch: 426 \t train loss: 9.218746527039912e-06\n",
      "Epoch: 427 \t train loss: 9.25848235056037e-06\n",
      "Epoch: 428 \t train loss: 9.25848235056037e-06\n",
      "Epoch: 429 \t train loss: 9.25848235056037e-06\n",
      "Epoch: 430 \t train loss: 9.258483260055073e-06\n",
      "Epoch: 431 \t train loss: 9.258483260055073e-06\n",
      "Epoch: 432 \t train loss: 9.258483260055073e-06\n",
      "Epoch: 433 \t train loss: 9.298219083575532e-06\n",
      "Epoch: 434 \t train loss: 9.298219083575532e-06\n",
      "Epoch: 435 \t train loss: 9.258484169549774e-06\n",
      "Epoch: 436 \t train loss: 9.258484169549774e-06\n",
      "Epoch: 437 \t train loss: 9.298219993070234e-06\n",
      "Epoch: 438 \t train loss: 9.298219993070234e-06\n",
      "Epoch: 439 \t train loss: 9.298219993070234e-06\n",
      "Epoch: 440 \t train loss: 9.298221812059637e-06\n",
      "Epoch: 441 \t train loss: 9.298221812059637e-06\n",
      "Epoch: 442 \t train loss: 9.298221812059637e-06\n",
      "Epoch: 443 \t train loss: 9.337957635580096e-06\n",
      "Epoch: 444 \t train loss: 9.337957635580096e-06\n",
      "Epoch: 445 \t train loss: 9.337957635580096e-06\n",
      "Epoch: 446 \t train loss: 9.337958545074798e-06\n",
      "Epoch: 447 \t train loss: 9.337958545074798e-06\n",
      "Epoch: 448 \t train loss: 9.377694368595257e-06\n",
      "Epoch: 449 \t train loss: 9.377694368595257e-06\n",
      "Epoch: 450 \t train loss: 9.377694368595257e-06\n",
      "Epoch: 451 \t train loss: 9.37769527808996e-06\n",
      "Epoch: 452 \t train loss: 9.37769527808996e-06\n",
      "Epoch: 453 \t train loss: 9.37769527808996e-06\n",
      "Epoch: 454 \t train loss: 9.41743201110512e-06\n",
      "Epoch: 455 \t train loss: 9.41743201110512e-06\n",
      "Epoch: 456 \t train loss: 9.377696187584661e-06\n",
      "Epoch: 457 \t train loss: 9.417432920599822e-06\n",
      "Epoch: 458 \t train loss: 9.417432920599822e-06\n",
      "Epoch: 459 \t train loss: 9.457168744120281e-06\n",
      "Epoch: 460 \t train loss: 9.457168744120281e-06\n",
      "Epoch: 461 \t train loss: 9.457168744120281e-06\n",
      "Epoch: 462 \t train loss: 9.457169653614983e-06\n",
      "Epoch: 463 \t train loss: 9.457169653614983e-06\n",
      "Epoch: 464 \t train loss: 9.457169653614983e-06\n",
      "Epoch: 465 \t train loss: 9.496906386630144e-06\n",
      "Epoch: 466 \t train loss: 9.496906386630144e-06\n",
      "Epoch: 467 \t train loss: 9.496907296124846e-06\n",
      "Epoch: 468 \t train loss: 9.496907296124846e-06\n",
      "Epoch: 469 \t train loss: 9.496907296124846e-06\n",
      "Epoch: 470 \t train loss: 9.536643119645305e-06\n",
      "Epoch: 471 \t train loss: 9.536643119645305e-06\n",
      "Epoch: 472 \t train loss: 9.576379852660466e-06\n",
      "Epoch: 473 \t train loss: 9.536644029140007e-06\n",
      "Epoch: 474 \t train loss: 9.576379852660466e-06\n",
      "Epoch: 475 \t train loss: 9.576379852660466e-06\n",
      "Epoch: 476 \t train loss: 9.576379852660466e-06\n",
      "Epoch: 477 \t train loss: 9.616116585675627e-06\n",
      "Epoch: 478 \t train loss: 9.616116585675627e-06\n",
      "Epoch: 479 \t train loss: 9.616117495170329e-06\n",
      "Epoch: 480 \t train loss: 9.616117495170329e-06\n",
      "Epoch: 481 \t train loss: 9.655853318690788e-06\n",
      "Epoch: 482 \t train loss: 9.655853318690788e-06\n",
      "Epoch: 483 \t train loss: 9.655853318690788e-06\n",
      "Epoch: 484 \t train loss: 9.695589142211247e-06\n",
      "Epoch: 485 \t train loss: 9.65585422818549e-06\n",
      "Epoch: 486 \t train loss: 9.695590051705949e-06\n",
      "Epoch: 487 \t train loss: 9.695590051705949e-06\n",
      "Epoch: 488 \t train loss: 9.73532678472111e-06\n",
      "Epoch: 489 \t train loss: 9.73532678472111e-06\n",
      "Epoch: 490 \t train loss: 9.77506260824157e-06\n",
      "Epoch: 491 \t train loss: 9.73532678472111e-06\n",
      "Epoch: 492 \t train loss: 9.73532678472111e-06\n",
      "Epoch: 493 \t train loss: 9.775063517736271e-06\n",
      "Epoch: 494 \t train loss: 9.775063517736271e-06\n",
      "Epoch: 495 \t train loss: 9.814800250751432e-06\n",
      "Epoch: 496 \t train loss: 9.814800250751432e-06\n",
      "Epoch: 497 \t train loss: 9.854536074271891e-06\n",
      "Epoch: 498 \t train loss: 9.814800250751432e-06\n",
      "Epoch: 499 \t train loss: 9.854536074271891e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in dataset:\n",
    "        model.last_hx = None\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), .1)\n",
    "        nn.utils.clip_grad\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_loss /= len(dataset)\n",
    "\n",
    "    print(\"Epoch: {} \\t train loss: {}\".format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def generate(self, idx, max_new_tokens = 100, temperature = 1.0, top_k = None, eos_token_id = None):\n",
    "    idx = idx.to(next(self.parameters()).device)\n",
    "    if len(idx[-1, :]) == 0:\n",
    "        return idx\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        self.last_hx = None\n",
    "        _idx = idx.clone()\n",
    "        logits = self(_idx)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        \n",
    "        next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "        if eos_token_id is not None and eos_token_id == next_token:\n",
    "            break\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def prompt(self, prompt, tokenizer, max_new_tokens = 100):\n",
    "    x = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    return tokenizer.decode(generate(self, x, max_new_tokens, 1.0, None, tokenizer.eos_token_id).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello;bye<|endoftext|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt(model, \"Hello\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['embedding.weight', 'lstm.cells.0.ih.weight', 'lstm.cells.0.ih.bias', 'lstm.cells.0.hh.weight', 'lstm.cells.0.hh.bias', 'lstm.cells.1.ih.weight', 'lstm.cells.1.ih.bias', 'lstm.cells.1.hh.weight', 'lstm.cells.1.hh.bias', 'out_proj.weight', 'out_proj.bias'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export.lstm_for_nlp(model).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.export(export.lstm_for_nlp(model), \"model.mdl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
